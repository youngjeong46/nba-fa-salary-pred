{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "I am acting as an NBA consultant; based on the 3-year performances of players before becoming Free Agents and the contract they ended up signing (calculated per year), I want to predict how this year's free agents will do (the 2018-19 season is only 1-2 games from being over). Teams targeting certain free agents in the Summer of '19 will be able to use this to determine who they want to target as their main pursuit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain the Data\n",
    "\n",
    "*Describe your data sources here and explain why they are relevant to the problem you are trying to solve.*\n",
    "\n",
    "First I will scrape data from basketball-reference.com that has player's individual statitstics per season from 2008-2009 to 2017-18 seasons. This will contain data with various player statistics that I can either use as my features or \n",
    "\n",
    "I will also scrape free agent lists from 2011 to 2018 seasons that will help me filter out non-impending free agents. This along with the salary they signed up for is available on spotrac.com. This data will help me collect \"y\".\n",
    "\n",
    "*After completing this step, be sure to edit `references/data_dictionary` to include descriptions of where you obtained your data and what information it contains.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/data/make_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/data/make_dataset.py\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException       \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "def check_exists(driver,classname):\n",
    "    try:\n",
    "        driver.find_element_by_class_name(classname)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def initialize_selenium(URL):\n",
    "    # initialize selenium\n",
    "    chromedriver = \"/Applications/chromedriver\" \n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(URL)\n",
    "    \n",
    "    return driver  \n",
    "\n",
    "# Generate dictionary to store our data per year\n",
    "def data_to_dict(years):\n",
    "    \"\"\"\n",
    "    Generate Dictionary that will store our data per year in this format:\n",
    "    \n",
    "    Key (Year): Value (Data)\n",
    "    \n",
    "    years: int indicating how many years of data will be stored\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    CURRENT_YEAR = int(datetime.now().year)\n",
    "    years_label = range(CURRENT_YEAR-1,CURRENT_YEAR-years,-1)\n",
    "    \n",
    "    return years_label, data\n",
    "    \n",
    "def download_salary_data(URL,years):\n",
    "    \n",
    "    years_label, data = data_to_dict(years)\n",
    "    driver = initialize_selenium(URL)\n",
    "\n",
    "    for i in in years_label:\n",
    "        time.sleep(2)\n",
    "        df = pd.read_html(driver.current_url)[0] \n",
    "        while check_exists(driver,'jcarousel-next-disabled') == False:\n",
    "            next = driver.find_element_by_class_name('jcarousel-next')\n",
    "            next.click()\n",
    "            time.sleep(5)\n",
    "            temp = pd.read_html(driver.current_url)[0]\n",
    "            df = df.append(temp,ignore_index=True)\n",
    "        years = Select(driver.find_element_by_class_name('tablesm'))\n",
    "        years.select_by_visible_text(str(i-1)+\"-\"+str(i))\n",
    "        data[i]=df\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def download_rookie_data(URL, years):\n",
    "    \n",
    "    years_label, data = data_to_dict(years)\n",
    "    driver = initialize_selenium(URL)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    for i in years_label:\n",
    "        df = pd.read_html(driver.current_url)[0]\n",
    "        df.columns=df.columns.droplevel()\n",
    "        df = df[['Player']]\n",
    "        data[i]=df\n",
    "        prev_year = driver.find_element_by_css_selector(\"a.button2.prev\")\n",
    "        prev_year.click()\n",
    "        time.sleep(10)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    \n",
    "def download_player_data(URL, years, type_data):\n",
    "    \n",
    "    years_label, data = data_to_dict(years)\n",
    "    driver = initialize_selenium(URL)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    # get to the current season stats, this may have changed\n",
    "    tab = driver.find_elements_by_id(\"header_leagues\")\n",
    "    hover = ActionChains(driver).move_to_element(tab[0])\n",
    "    hover.perform()\n",
    "    wait.until(EC.visibility_of_element_located((By.LINK_TEXT, type_data))).click()\n",
    "    \n",
    "    for i in years_label:\n",
    "        df = pd.read_html(driver.current_url)[0]\n",
    "        df = df[df.Rk != 'Rk']\n",
    "        data[i]=df\n",
    "        prev_year = driver.find_element_by_css_selector(\"a.button2.prev\")\n",
    "        prev_year.click()\n",
    "        time.sleep(10)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def download_fa_data(URL):\n",
    "    years_label, data = data_to_dict(years)\n",
    "    driver = initialize_selenium(URL)\n",
    "\n",
    "    for i in range(2018,2010,-1):\n",
    "        years = Select(driver.find_element_by_name('year'))\n",
    "        years.select_by_visible_text(str(i))\n",
    "        submit = driver.find_element_by_class_name('go')\n",
    "        submit.click()\n",
    "        time.sleep(10)\n",
    "        df = pd.read_html(driver.current_url)[0]\n",
    "        data[i]=df\n",
    "        \n",
    "    return data\n",
    "\n",
    "def save_dataset(data,filename):\n",
    "    with open(filename, 'wb') as w:\n",
    "        pickle.dump(data,w)\n",
    "        \n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that download data from one or more sources\n",
    "    and saves those datasets to the data/raw directory.\n",
    "    \"\"\"\n",
    "    data_fa = download_fa_data(\"https://www.spotrac.com/nba/free-agents/\")\n",
    "    data_reg = download_player_data(\"https://www.basketball-reference.com\", 12, \"Per G\")\n",
    "    data_adv = download_player_data(\"https://www.basketball-reference.com\", 12, \"Advanced\")\n",
    "    data_salary = download_salary_data(\"http://www.espn.com/nba/salaries/_/year/2019\", 12)\n",
    "    data_rookie = download_rookie_data(\"https://www.basketball-reference.com/leagues/NBA_2018_rookies.html\", 12)\n",
    "    save_dataset(data_fa, \"data/raw/datafa.pickle\")\n",
    "    save_dataset(data_reg, \"data/raw/regstats.pickle\")\n",
    "    save_dataset(data_adv, \"data/raw/advstats.pickle\")\n",
    "    save_dataset(data_salary, \"data/raw/salaries.pickle\")\n",
    "    save_dataset(data_rookie, \"data/raw/rookies.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub the Data\n",
    "\n",
    "*Look through the raw data files and see what you will need to do to them in order to have a workable data set. If your source data is already well-formatted, you may want to ask yourself why it hasn't already been analyzed and what other people may have overlooked when they were working on it. Are there other data sources that might give you more insights on some of the data you have here?*\n",
    "\n",
    "*The end goal of this step is to produce a [design matrix](https://en.wikipedia.org/wiki/Design_matrix), containing one column for every variable that you are modeling, including a column for the outputs, and one row for every observation in your data set. It needs to be in a format that won't cause any problems as you visualize and model your data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/features/build_features.py\n",
    "\n",
    "# imports\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "# helper functions go here\n",
    "def clean_salaries_dataset(path, filename):\n",
    "    money = pickle.load(open(path+\"/\"+filename, \"rb\"))\n",
    "\n",
    "def clean_stats_dataset(path, filename1, filename2):\n",
    "    stats = pickle.load(open(path+\"/\"+filename2, \"rb\"))\n",
    "    advs = pickle.load(open(path+\"/\"+filename, \"rb\"))\n",
    "\n",
    "def clean_rookie_dataset(path, filename):\n",
    "    rookies = pickle.load(open(path+\"/\"+filename, \"rb\"))\n",
    "    \n",
    "def clean_fa_dataset(path, filename):\n",
    "    freeagents = pickle.load(open(path+\"/\"+filename, \"rb\"))\n",
    "    \n",
    "def build_dataset(salaries, stats, rookies, freeagents):\n",
    "    \n",
    "    return data\n",
    "\n",
    "def build_features(data):\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_features(data,filename):\n",
    "    with open(filename,\"wb\") as writer:\n",
    "        pickle.dump(data,writer)\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/raw, cleans them,\n",
    "    and converts the data into a design matrix that is ready for modeling.\n",
    "    \"\"\"\n",
    "    salaries = clean_salaries_dataset('/data/raw', \"salaries.pickle\")\n",
    "    stats = clean_stats_dataset('data/raw', \"advstats.pickle\", \"regstats.pickle\")\n",
    "    rookies = clean_rookies_dataset('data/raw','rookies.pickle')\n",
    "    freeagents = clean_fa_dataset('data/raw','datafa.pickle')\n",
    "    \n",
    "    full_data = build_dataset(salaries, stats, rookies, freeagents)\n",
    "    \n",
    "    build_features(data)\n",
    "    \n",
    "    # save_features(data,'data/processed/data.pickle')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before moving on to exploratory analysis, write down some notes about challenges encountered while working with this data that might be helpful for anyone else (including yourself) who may work through this later on.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data\n",
    "\n",
    "*Before you start exploring the data, write out your thought process about what you're looking for and what you expect to find. Take a minute to confirm that your plan actually makes sense.*\n",
    "\n",
    "*Calculate summary statistics and plot some charts to give you an idea what types of useful relationships might be in your dataset. Use these insights to go back and download additional data or engineer new features if necessary. Not now though... remember we're still just trying to finish the MVP!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/visualization/visualize.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/processed,\n",
    "    calculates descriptive statistics for the population, and plots charts\n",
    "    that visualize interesting relationships between features.\n",
    "    \"\"\"\n",
    "    # data = load_features('data/processed')\n",
    "    # describe_features(data, 'reports/')\n",
    "    # generate_charts(data, 'reports/figures/')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What did you learn? What relationships do you think will be most helpful as you build your model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the Data\n",
    "\n",
    "*Describe the algorithm or algorithms that you plan to use to train with your data. How do these algorithms work? Why are they good choices for this data and problem space?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/models/train_model.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/processed,\n",
    "    calculates descriptive statistics for the population, and plots charts\n",
    "    that visualize interesting relationships between features.\n",
    "    \"\"\"\n",
    "    # data = load_features('data/processed/')\n",
    "    # train, test = train_test_split(data)\n",
    "    # save_train_test(train, test, 'data/processed/')\n",
    "    # model = build_model()\n",
    "    # model.fit(train)\n",
    "    # save_model(model, 'models/')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/models/predict_model.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/processed,\n",
    "    calculates descriptive statistics for the population, and plots charts\n",
    "    that visualize interesting relationships between features.\n",
    "    \"\"\"\n",
    "    # test_X, test_y = load_test_data('data/processed')\n",
    "    # trained_model = load_model('models/')\n",
    "    # predictions = trained_model.predict(test_X)\n",
    "    # metrics = evaluate(test_y, predictions)\n",
    "    # save_metrics('reports/')\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write down any thoughts you may have about working with these algorithms on this data. What other ideas do you want to try out as you iterate on this pipeline?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret the Model\n",
    "\n",
    "_Write up the things you learned, and how well your model performed. Be sure address the model's strengths and weaknesses. What types of data does it handle well? What types of observations tend to give it a hard time? What future work would you or someone reading this might want to do, building on the lessons learned and tools developed in this project?_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
